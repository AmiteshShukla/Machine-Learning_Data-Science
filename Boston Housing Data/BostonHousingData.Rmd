---
title: "BostonHousingData"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
suppressMessages(library(glmnet))  # glmnet for lasso
suppressMessages(library(ggplot2))  # qplot
suppressMessages(library(gridExtra))  # grid.arrange,
suppressMessages(library(tidyr))
suppressMessages(library(reshape2))
suppressMessages(library(plyr))
set.seed(9661)
rm(list = ls())  # remove the objects in R before loading the new data
load('BostonHousing1.Rdata', verbose = TRUE) # The data is loaded ans object Housing1
myData = Housing1
n = nrow(myData)
p = ncol(myData) - 1

# some algorithms need the matrix/vector 
# input (instead of a data frame)
X = data.matrix(myData[,-1])  
Y = myData[,1]
mspe = matrix(data = 0,nrow = 50, ncol = 10)
df = matrix(data = 0,nrow = 50, ncol = 10)
rTime = matrix(data = 0,nrow = 1, ncol = 10)
for(i in 1:50) {
# all.test.id: ntestxT matrix, each column records 
ntest = round(n * 0.25)  # test set size
ntrain = n-ntest  # training set size
all.test.id = matrix(0, ntest, T)  # 
for(t in 1:T){
  all.test.id[, t] = sample(1:n, ntest)
}
save(all.test.id, file="alltestID.RData")

test.id = all.test.id[,1]
# Full Method

  start.time = proc.time()
  full.model = lm(Y ~ ., data = myData[-test.id,])
  Ytest.pred = predict(full.model, newdata = myData[test.id,])
  df[i,5] = length(full.model$coefficients) - 1
  mspe[i,5] = mean((Y[test.id] - Ytest.pred)^2)
  etime<- proc.time() - start.time
  rTime [1,1] = rTime [1,1] + etime[3]

# Forward AIC

start.time = proc.time()
full.model = lm(Y ~ ., data = myData[-test.id, ])
stepAIC = step(lm(Y ~ 1, data = myData[-test.id, ]), 
                 list(upper = full.model),
                 trace = 0, direction = "forward")
Ytest.pred = predict(stepAIC, newdata = myData[test.id, ])

# number of predictors (excluding the intercept)
df[i,2] =length(stepAIC$coef) - 1  
etime<- proc.time() - start.time
  rTime [1,2] = rTime [1,2] + etime[3]

  mspe[i,2] = mean((Y[test.id] - Ytest.pred)^2)

#Backward AIC

start.time = proc.time()
full.model = lm(Y ~ ., data = myData[-test.id, ])
stepAIC = step(full.model, trace = 0, direction = "backward")
Ytest.pred = predict(stepAIC, newdata = myData[test.id, ])
df[i,1] =length(stepAIC$coef) - 1
etime<- proc.time() - start.time
  rTime [1,3] = rTime [1,3] + etime[3]

  mspe[i,1] = mean((Y[test.id] - Ytest.pred)^2)

#Forward BIC

start.time = proc.time()
full.model = lm(Y ~ ., data = myData[-test.id, ])
stepAIC = step(lm(Y ~ 1, data = myData[-test.id, ]),
               list(upper = full.model),
               trace = 0, direction = "forward", k = log(ntrain))
Ytest.pred = predict(stepAIC, newdata = myData[test.id, ])

# number of predictors (excluding the intercept)
df[i,4] =length(stepAIC$coef) - 1 
etime<- proc.time() - start.time
  rTime [1,4] = rTime [1,4] + etime[3]

  mspe[i,4] = mean((Y[test.id] - Ytest.pred)^2)

# Backward BIC

start.time = proc.time()
full.model = lm(Y ~ ., data = myData[-test.id, ])
stepAIC = step(full.model, trace = 0,
               direction = "backward", k = log(ntrain))
Ytest.pred = predict(stepAIC, newdata = myData[test.id, ])
df[i,3] =length(stepAIC$coef) - 1
etime<- proc.time() - start.time
  rTime [1,5] = rTime [1,5] + etime[3]

  mspe[i,3] = mean((Y[test.id] - Ytest.pred)^2)
  
  

#Ridge with lambda_min/lambda_1se (R_min, R_1se)

start.time = proc.time()
lambda <- 10^seq(5, -8, by = -.1)
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = lambda)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,6] = rTime [1,6] + etime[3]

mspe[i,10] = mean((Ytest.pred - Y[test.id])^2)

start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = lambda)

best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,7] = rTime [1,7] + etime[3]
mspe[i,9] = mean((Ytest.pred - Y[test.id])^2)

# Exclude the computation for DF when recording computing time for Ridge,

# DF for Ridge
# DF = tr(S) where S = X(X^t X + lam*I)^{-1}X^t and X is the standardized design 
# matrix with each column having mean zero and sd 1. Note that glmnet computes
# sd using denominator n not (n-1). 
# In addition, the objective function in glmnet for ridge is slightly different
# from the one used in class. So lam used in S (above) corresponds to lambda.min
# or lambda.1se times the sample size

ntrain = n - dim(all.test.id)[1]
tmpX = scale(X[-test.id, ]) * sqrt(ntrain / (ntrain - 1))
d = svd(tmpX)$d 

# df for Ridge with lambda_min
best.lam = cv.out$lambda.min
df[i,10] = sum(d^2/(d^2 + best.lam*ntrain))

# df for Ridge with lambda_1se
best.lam = cv.out$lambda.1se
df[i,9] = sum(d^2/(d^2 + best.lam*ntrain))

# Lasso using lambda.min (L_min)
start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,8] = rTime [1,8] + etime[3]
mspe[i,7] = mean((Ytest.pred - Y[test.id])^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
df[i,7] = sum(mylasso.coef != 0) - 1  # size of Lasso with lambda.min

#Lasso using lambda.1se without/with Refit (L_1se, L_Refit)
start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,9] = rTime [1,9] + etime[3]
mspe[i,6] = mean((Ytest.pred - Y[test.id])^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
df[i,6] = sum(mylasso.coef != 0) - 1 # size of Lasso with lambda.1se

start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.1se
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[nonzeroCoef(mylasso.coef)[-1]]
tmp.X = X[, colnames(X) %in% var.sel]
mylasso.refit = coef(lm(Y[-test.id] ~ tmp.X[-test.id, ]))
df[i,8] = length(mylasso.refit) - 1
Ytest.pred = mylasso.refit[1] + tmp.X[test.id,] %*% mylasso.refit[-1]
etime<- proc.time() - start.time
  rTime [1,10] = rTime [1,10] + etime[3]
mspe[i,8] = mean((Ytest.pred - Y[test.id])^2)
}
```

```{r, echo= FALSE}
datamse<-as.data.frame(mspe)
colnames(datamse) <- c("AIC-B","AIC-F","BIC-B","BIC-F", "Full", "L1se", "L_min", "L_Refit", "R_1se", "R_min")

d1<-datamse %>% gather(method, error) %>% mutate(method = factor(method, levels = c("AIC-B","AIC-F","BIC-B","BIC-F", "Full", "L1se", "L_min", "L_Refit", "R_1se", "R_min"))) %>% 
  ggplot(aes(method, error)) + 
    geom_boxplot(aes(colour = method)) + labs(x = "method", y = "Prediction Errors")
plot1<- d1 + theme(legend.position="none")
```

```{r, echo= FALSE}
dfe<-as.data.frame(df)
colnames(dfe) <- c("AIC-B","AIC-F","BIC-B","BIC-F", "Full", "L1se", "L_min", "L_Refit", "R_1se", "R_min")

d2<-dfe %>% gather(method, size) %>% mutate(method = factor(method, levels = c("AIC-B","AIC-F","BIC-B","BIC-F", "Full", "L1se", "L_min", "L_Refit", "R_1se", "R_min"))) %>% 
  ggplot(aes(method, size)) + 
    geom_boxplot(aes(colour = method)) + labs(x = "method", y = "Model Size")
plot2<- d2 + theme(legend.position="none")
```

```{r, echo= FALSE}
grid.arrange(plot1, plot2)
rTime<- as.data.frame(rTime)
colnames(rTime) <- c("Full", "AIC-F","AIC-B","BIC-F","BIC-B", "L1se", "L_min", "L_Refit", "R_1se", "R_min")
```



```{r}
#Computation time for Problem#1
rTime
```




```{r, echo=FALSE}
set.seed(9661)
rm(list = ls())  # remove the objects in R before loading the new data
load('BostonHousing2.Rdata', verbose = TRUE) # The data is loaded ans object Housing2
myData = Housing2
n = nrow(myData)
p = ncol(myData) - 1

# some algorithms need the matrix/vector 
# input (instead of a data frame)
X = data.matrix(myData[,-1])  
Y = myData[,1]
mspe = matrix(data = 0,nrow = 50, ncol = 5)
df = matrix(data = 0,nrow = 50, ncol = 5)
rTime = matrix(data = 0,nrow = 1, ncol = 5)
for(i in 1:50) {
# all.test.id: ntestxT matrix, each column records 
ntest = round(n * 0.25)  # test set size
ntrain = n-ntest  # training set size
all.test.id = matrix(0, ntest, T)  # 
for(t in 1:T){
  all.test.id[, t] = sample(1:n, ntest)
}
save(all.test.id, file="alltestID.RData")

test.id = all.test.id[,1]

#Ridge with lambda_min/lambda_1se (R_min, R_1se)

start.time = proc.time()
lambda <- 10^seq(5, -8, by = -.1)
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = lambda)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,1] = rTime [1,1] + etime[3]

mspe[i,5] = mean((Ytest.pred - Y[test.id])^2)

start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = lambda)

best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,2] = rTime [1,2] + etime[3]
mspe[i,4] = mean((Ytest.pred - Y[test.id])^2)

ntrain = n - dim(all.test.id)[1]
tmpX = scale(X[-test.id, ]) * sqrt(ntrain / (ntrain - 1))
d = svd(tmpX)$d 

# df for Ridge with lambda_min
best.lam = cv.out$lambda.min
df[i,5] = sum(d^2/(d^2 + best.lam*ntrain))

# df for Ridge with lambda_1se
best.lam = cv.out$lambda.1se
df[i,4] = sum(d^2/(d^2 + best.lam*ntrain))

# Lasso using lambda.min (L_min)
start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,3] = rTime [1,3] + etime[3]
mspe[i,2] = mean((Ytest.pred - Y[test.id])^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
df[i,2] = sum(mylasso.coef != 0) - 1  # size of Lasso with lambda.min

#Lasso using lambda.1se without/with Refit (L_1se, L_Refit)
start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,4] = rTime [1,4] + etime[3]
mspe[i,1] = mean((Ytest.pred - Y[test.id])^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
df[i,1] = sum(mylasso.coef != 0) - 1 # size of Lasso with lambda.1se

start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.1se
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[nonzeroCoef(mylasso.coef)[-1]]
tmp.X = X[, colnames(X) %in% var.sel]
mylasso.refit = coef(lm(Y[-test.id] ~ tmp.X[-test.id, ]))
df[i,3] = length(mylasso.refit) - 1
Ytest.pred = mylasso.refit[1] + tmp.X[test.id,] %*% mylasso.refit[-1]
etime<- proc.time() - start.time
  rTime [1,5] = rTime [1,5] + etime[3]
mspe[i,3] = mean((Ytest.pred - Y[test.id])^2)
}
```

```{r, echo= FALSE}
datamse<-as.data.frame(mspe)
colnames(datamse) <- c("L1se", "L_min", "L_Refit", "R_1se", "R_min")

d1<-datamse %>% gather(method, error) %>% mutate(method = factor(method, levels = c("L1se", "L_min", "L_Refit", "R_1se", "R_min"))) %>% 
  ggplot(aes(method, error)) + 
    geom_boxplot(aes(colour = method)) + labs(x = "method", y = "Prediction Errors")
plot1<- d1 + theme(legend.position="none")
```

```{r, echo= FALSE}
dfe<-as.data.frame(df)
colnames(dfe) <- c("L1se", "L_min", "L_Refit", "R_1se", "R_min")

d2<-dfe %>% gather(method, size) %>% mutate(method = factor(method, levels = c("L1se", "L_min", "L_Refit", "R_1se", "R_min"))) %>% 
  ggplot(aes(method, size)) + 
    geom_boxplot(aes(colour = method)) + labs(x = "method", y = "Model Size")
plot2<- d2 + theme(legend.position="none")
```

```{r, echo= FALSE}
grid.arrange(plot1, plot2)
rTime<- as.data.frame(rTime)
colnames(rTime) <- c("R_min", "R_1se", "L_min", "L1se","L_Refit")
```

```{r}
#Computation time for Problem#2
rTime
```


```{r, echo=FALSE}
set.seed(9661)
rm(list = ls())  # remove the objects in R before loading the new data
load('BostonHousing3.Rdata', verbose = TRUE) # The data is loaded ans object Housing2
myData = Housing3
n = nrow(myData)
p = ncol(myData) - 1

# some algorithms need the matrix/vector 
# input (instead of a data frame)
X = data.matrix(myData[,-1])  
Y = myData[,1]
mspe = matrix(data = 0,nrow = 50, ncol = 5)
df = matrix(data = 0,nrow = 50, ncol = 5)
rTime = matrix(data = 0,nrow = 1, ncol = 5)
for(i in 1:50) {
# all.test.id: ntestxT matrix, each column records 
ntest = round(n * 0.25)  # test set size
ntrain = n-ntest  # training set size
all.test.id = matrix(0, ntest, T)  # 
for(t in 1:T){
  all.test.id[, t] = sample(1:n, ntest)
}
save(all.test.id, file="alltestID.RData")

test.id = all.test.id[,1]

#Ridge with lambda_min/lambda_1se (R_min, R_1se)

start.time = proc.time()
lambda <- 10^seq(5, -8, by = -.1)
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = lambda)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,1] = rTime [1,1] + etime[3]

mspe[i,5] = mean((Ytest.pred - Y[test.id])^2)

start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = lambda)

best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,2] = rTime [1,2] + etime[3]
mspe[i,4] = mean((Ytest.pred - Y[test.id])^2)

ntrain = n - dim(all.test.id)[1]
tmpX = scale(X[-test.id, ]) * sqrt(ntrain / (ntrain - 1))
d = svd(tmpX)$d 

# df for Ridge with lambda_min
best.lam = cv.out$lambda.min
df[i,5] = sum(d^2/(d^2 + best.lam*ntrain))

# df for Ridge with lambda_1se
best.lam = cv.out$lambda.1se
df[i,4] = sum(d^2/(d^2 + best.lam*ntrain))

# Lasso using lambda.min (L_min)
start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,3] = rTime [1,3] + etime[3]
mspe[i,2] = mean((Ytest.pred - Y[test.id])^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
df[i,2] = sum(mylasso.coef != 0) - 1  # size of Lasso with lambda.min

#Lasso using lambda.1se without/with Refit (L_1se, L_Refit)
start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
etime<- proc.time() - start.time
  rTime [1,4] = rTime [1,4] + etime[3]
mspe[i,1] = mean((Ytest.pred - Y[test.id])^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
df[i,1] = sum(mylasso.coef != 0) - 1 # size of Lasso with lambda.1se

start.time = proc.time()
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1, lambda = lambda)
best.lam = cv.out$lambda.1se
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[nonzeroCoef(mylasso.coef)[-1]]
tmp.X = X[, colnames(X) %in% var.sel]
mylasso.refit = coef(lm(Y[-test.id] ~ tmp.X[-test.id, ]))
df[i,3] = length(mylasso.refit) - 1
Ytest.pred = mylasso.refit[1] + tmp.X[test.id,] %*% mylasso.refit[-1]
etime<- proc.time() - start.time
  rTime [1,5] = rTime [1,5] + etime[3]
mspe[i,3] = mean((Ytest.pred - Y[test.id])^2)
}
```

```{r, echo= FALSE}
datamse<-as.data.frame(mspe)
colnames(datamse) <- c("L1se", "L_min", "L_Refit", "R_1se", "R_min")

d1<-datamse %>% gather(method, error) %>% mutate(method = factor(method, levels = c("L1se", "L_min", "L_Refit", "R_1se", "R_min"))) %>% 
  ggplot(aes(method, error)) + 
    geom_boxplot(aes(colour = method)) + labs(x = "method", y = "Prediction Errors")
plot1<- d1 + theme(legend.position="none")
```

```{r, echo= FALSE}
dfe<-as.data.frame(df)
colnames(dfe) <- c("L1se", "L_min", "L_Refit", "R_1se", "R_min")

d2<-dfe %>% gather(method, size) %>% mutate(method = factor(method, levels = c("L1se", "L_min", "L_Refit", "R_1se", "R_min"))) %>% 
  ggplot(aes(method, size)) + 
    geom_boxplot(aes(colour = method)) + labs(x = "method", y = "Model Size")
plot2<- d2 + theme(legend.position="none")
```

```{r, echo= FALSE}
grid.arrange(plot1, plot2)
rTime<- as.data.frame(rTime)
colnames(rTime) <- c("R_min", "R_1se", "L_min", "L1se","L_Refit")
```

```{r}
#Computation time for Problem#3
rTime
```